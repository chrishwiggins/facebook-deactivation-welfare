{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35q65mmoe8r",
   "source": "# The Welfare Effects of Social Media: Exploring the Facebook Deactivation Experiment\n\n**Persuasion at Scale** (PSAM 3707 / UN 3707), Week 4\n\nBased on: Allcott, Braghieri, Eichmeyer, and Gentzkow (2020). \"The Welfare Effects of Social Media.\" *American Economic Review* 110(3): 629-676.\n\n---\n\n## What happened in this experiment?\n\nIn October 2018, right before the US midterm elections, researchers **paid Facebook users to deactivate their accounts for four weeks**.\n\n- 2,743 users recruited via Facebook ads\n- Those willing to deactivate for $102 or less were randomized\n- ~830 in the Treatment group (paid to deactivate)\n- ~830 in the Control group (kept using Facebook)\n- Over 90% compliance with deactivation\n\n**The big questions:** What happens when you take away someone's Facebook? Do they become happier? Less informed? Less politically polarized?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3c30oiyxvh",
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 13\nnp.random.seed(2018)  # the year of the experiment",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qqlq7yq1yt",
   "source": "## Part 1: The Experimental Design\n\nBefore we look at results, let's understand who was in this experiment and how randomization works.\n\n### Who were the participants?\n\nThe sample is **not** a random sample of Americans. It's a sample of Facebook users who:\n1. Saw a Facebook ad about the study\n2. Were willing to participate\n3. Were willing to deactivate for $102 or less\n\nThis matters! Let's see how they compare to the general population.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3txvp7q5yr7",
   "source": "# Table 2 from the paper: Sample vs. population demographics\ncategories = ['Income\\nunder $50K', 'College\\neducated', 'Male', 'White', 'Age\\nunder 30', 'Republican', 'Democrat']\nsample_vals = [0.40, 0.51, 0.43, 0.68, 0.52, 0.13, 0.42]\nfb_users =   [0.41, 0.33, None, None, None, None, None]  # limited public data\nus_pop =     [0.52, 0.30, 0.49, 0.64, 0.21, 0.26, 0.31]\n\nfig, ax = plt.subplots(figsize=(12, 6))\nx = np.arange(len(categories))\nwidth = 0.3\n\nbars1 = ax.bar(x - width, sample_vals, width, label='Experiment sample', color='#2196F3', alpha=0.85)\nbars3 = ax.bar(x + width, us_pop, width, label='US population', color='#9E9E9E', alpha=0.7)\n\n# Add FB user bars where available\nfor i, val in enumerate(fb_users):\n    if val is not None:\n        ax.bar(x[i], val, width, color='#4CAF50', alpha=0.7)\nax.bar([], [], width, color='#4CAF50', alpha=0.7, label='Facebook users')\n\nax.set_ylabel('Proportion')\nax.set_title('Who volunteered for a Facebook deactivation experiment?\\n(Table 2 from Allcott et al. 2020)', fontsize=15)\nax.set_xticks(x)\nax.set_xticklabels(categories)\nax.legend(loc='upper right')\nax.set_ylim(0, 0.75)\n\n# Annotate the key differences\nax.annotate('Much more\\nDemocratic', xy=(6, 0.42), xytext=(6, 0.60),\n            fontsize=10, ha='center', color='#1565C0',\n            arrowprops=dict(arrowstyle='->', color='#1565C0'))\nax.annotate('Much younger', xy=(4, 0.52), xytext=(4.5, 0.65),\n            fontsize=10, ha='center', color='#1565C0',\n            arrowprops=dict(arrowstyle='->', color='#1565C0'))\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key takeaway: The sample skews young, educated, female, and Democratic.\")\nprint(\"This is important for interpreting the results (external validity).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "21ulalo1ijm",
   "source": "### How much were people willing to accept?\n\nBefore randomization, the researchers asked: *\"What's the minimum you'd accept to deactivate Facebook for 4 weeks?\"*\n\nThe distribution of these valuations tells us something about how much people value Facebook.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "8rdwjzaw51a",
   "source": "# Simulate the WTA distribution (matching paper: median ~$100, mean ~$180, right-skewed)\n# The paper reports that 61% had WTA <= $102\nn_total = 2743\nwta = np.concatenate([\n    np.random.lognormal(mean=3.8, sigma=0.9, size=int(n_total * 0.61)),  # those <= $102\n    np.random.lognormal(mean=5.5, sigma=0.8, size=int(n_total * 0.39))   # those > $102\n])\nwta = np.clip(wta, 1, 1000)\n# Adjust so ~61% are <= 102\nwta[wta > 102] = wta[wta > 102] * 1.5  # push high values further out\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: histogram of WTA\nax1.hist(wta, bins=50, color='#5C6BC0', alpha=0.8, edgecolor='white')\nax1.axvline(x=102, color='red', linestyle='--', linewidth=2, label='$102 cutoff')\nax1.axvline(x=np.median(wta), color='orange', linestyle='--', linewidth=2, label=f'Median: ${np.median(wta):.0f}')\nax1.set_xlabel('Willingness to Accept ($)')\nax1.set_ylabel('Number of participants')\nax1.set_title('How much would you need to quit Facebook\\nfor 4 weeks?')\nax1.legend()\nax1.set_xlim(0, 500)\n\n# Right: the randomization scheme\nlabels = ['Total\\nRecruited', 'WTA â‰¤ $102\\n(randomized)', 'Treatment\\n(deactivate)', 'Control\\n(keep FB)']\nvalues = [2743, 1661, 831, 830]\ncolors = ['#78909C', '#5C6BC0', '#EF5350', '#66BB6A']\n\nbars = ax2.barh(range(4), values, color=colors, height=0.6)\nax2.set_yticks(range(4))\nax2.set_yticklabels(labels)\nax2.set_xlabel('Number of participants')\nax2.set_title('The randomization funnel')\nax2.invert_yaxis()\nfor bar, val in zip(bars, values):\n    ax2.text(bar.get_width() + 30, bar.get_y() + bar.get_height()/2,\n             f'n = {val}', va='center', fontsize=12)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Only {1661/2743:.0%} of recruits were willing to deactivate for $102.\")\nprint(\"These are the people who value Facebook LESS. Keep that in mind.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h063ebgntoq",
   "source": "## Part 2: Loading the Real Data\n\nThe replication data is from [openICPSR project 112081](https://www.openicpsr.org/openicpsr/project/112081) (CC BY 4.0 license). We use the anonymized endline survey, which contains the main outcome measures for 2,823 participants.\n\n### The four families of outcomes\n\nThe paper measures effects in four domains:\n\n| Domain | What they measured | Effect of deactivation |\n|--------|-------------------|----------------------|\n| **Time use** | Minutes on Facebook, other activities | Freed up ~60 min/day |\n| **News & politics** | News quiz, polarization, engagement | Less informed, less polarized |\n| **Well-being** | Happiness, life satisfaction, depression | Slightly happier |\n| **Post-experiment** | Did they go back to Facebook? | Used Facebook less afterward |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ya562urjivd",
   "source": "# Load the real replication data from openICPSR (CC BY 4.0)\n# Data files are CSV exports from the original Stata .dta files\n\nDATA_URL = 'https://raw.githubusercontent.com/chrishwiggins/facebook-deactivation-welfare/main/data/'\n\nel = pd.read_csv(DATA_URL + 'endline.csv')\nbl = pd.read_csv(DATA_URL + 'baseline_slim.csv')\npe = pd.read_csv(DATA_URL + 'postendline.csv')\nsms = pd.read_csv(DATA_URL + 'sms_summary.csv')\n\n# t and fb_minutes are already numeric in the CSV\nel['t'] = pd.to_numeric(el['t'], errors='coerce')\nel['fb_minutes'] = pd.to_numeric(el['fb_minutes'], errors='coerce')\nel['id'] = el['id'].astype(str)\nbl['ID'] = bl['ID'].astype(str)\n\n# Parse happiness: \"5\", \"7 (a very happy person)\", etc. -> extract leading digit\nel['happiness'] = pd.to_numeric(\n    el['swb_happiness'].astype(str).str.extract(r'(\\d+)')[0], errors='coerce')\n\n# Life satisfaction (Likert text -> 1-7)\nswl_map = {'Strongly disagree': 1, 'Disagree': 2, 'Slightly disagree': 3,\n           'Neither agree nor disagree': 4, 'Slightly agree': 5, 'Agree': 6, 'Strongly agree': 7}\nfor col in ['swb_swl1', 'swb_swl2', 'swb_swl3']:\n    el[col + '_n'] = el[col].map(swl_map)\nel['life_satisfaction'] = el[['swb_swl1_n', 'swb_swl2_n', 'swb_swl3_n']].mean(axis=1)\n\n# Loneliness (text -> 1-3)\nlnlns_map = {'Hardly ever': 1, 'Some of the time': 2, 'Often': 3}\nfor col in ['swb_lnlns1', 'swb_lnlns2', 'swb_lnlns3']:\n    el[col + '_n'] = el[col].map(lnlns_map)\nel['loneliness'] = el[['swb_lnlns1_n', 'swb_lnlns2_n', 'swb_lnlns3_n']].mean(axis=1)\n\n# Depression items: \"3.\", \"4. All or almost all of the time.\" -> extract leading digit\nfor col in ['swb_eurhappsvy_4', 'swb_eurhappsvy_5', 'swb_eurhappsvy_6', 'swb_eurhappsvy_7']:\n    el[col + '_n'] = pd.to_numeric(\n        el[col].astype(str).str.extract(r'(\\d+)')[0], errors='coerce')\nel['depression'] = el[['swb_eurhappsvy_4_n', 'swb_eurhappsvy_5_n', \n                        'swb_eurhappsvy_6_n', 'swb_eurhappsvy_7_n']].mean(axis=1)\n\n# News knowledge: items are True/False/Unsure quiz responses.\n# Count confident (non-\"Unsure\") answers as a proxy for news engagement.\nnk_cols = [c for c in el.columns if c.startswith('news_knowledge')]\nfor c in nk_cols:\n    el[c + '_confident'] = (el[c] != 'Unsure').astype(float)\n    el.loc[el[c].isna(), c + '_confident'] = np.nan\nel['news_knowledge'] = el[[c + '_confident' for c in nk_cols]].sum(axis=1)\n\n# Attitude columns are already float64\nel['attitude_trump_1_n'] = el['attitude_trump_1']\n\n# Follow politics (text -> 1-4)\nfp_map = {'Not at all closely': 1, 'Somewhat closely': 2, \n          'Rather closely': 3, 'Very closely': 4}\nel['follow_politics_n'] = el['follow_politics'].map(fp_map)\n\n# WTA (already numeric)\nel['wta'] = pd.to_numeric(el['wta3'], errors='coerce')\nel.loc[el['wta'] > 10000, 'wta'] = np.nan\n\n# Merge baseline demographics\ndf = el.merge(bl[['ID', 'qualified', 'educ_prescreen', 'repdem', 'fb_minutes_prescreen']], \n              left_on='id', right_on='ID', how='left')\n\n# Create treatment label\ndf['treatment'] = df['t'].map({1: 'Deactivated', 0: 'Control'})\n\nn_treat = (df['t'] == 1).sum()\nn_ctrl = (df['t'] == 0).sum()\nprint(f\"Real data loaded: {len(df)} participants ({n_treat} treatment, {n_ctrl} control)\")\nprint(f\"\\nFacebook minutes/day:\")\nprint(f\"  Control:   {df.loc[df.t==0, 'fb_minutes'].mean():.1f} (sd={df.loc[df.t==0, 'fb_minutes'].std():.1f})\")\nprint(f\"  Treatment: {df.loc[df.t==1, 'fb_minutes'].mean():.1f} (sd={df.loc[df.t==1, 'fb_minutes'].std():.1f})\")\nprint(f\"\\nHappiness (1-7):\")\nprint(f\"  Control:   {df.loc[df.t==0, 'happiness'].mean():.2f}\")\nprint(f\"  Treatment: {df.loc[df.t==1, 'happiness'].mean():.2f}\")\nprint(f\"\\nNews knowledge (0-15, confident answers):\")\nprint(f\"  Control:   {df.loc[df.t==0, 'news_knowledge'].mean():.1f}\")\nprint(f\"  Treatment: {df.loc[df.t==1, 'news_knowledge'].mean():.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kq9rw8zv2i",
   "source": "## Wait: who's in this dataset?\n\nThe endline CSV has **2,823 rows**, but only ~1,660 of those were in the actual experiment. The rest are people with high willingness-to-accept (WTA > $102) who were **never randomized**. They were surveyed but not assigned to treatment or control through randomization.\n\nIf we include them in our analysis, they all end up in the \"control\" group, which **breaks the experiment**. These high-WTA users value Facebook more than the randomized participants, so lumping them in with the real controls biases every comparison.\n\nThis is a classic **sample selection** problem: the published paper analyzes only the experimental sample (baseline WTA $\\leq$ $102, and not flagged as low-quality responses). Let's see what happens when we do the same.\n\n### The lesson\n\nReplication data files often contain more observations than the paper's analysis sample. If you don't read the codebook and filter correctly, you can get **wrong signs** on your treatment effects. The randomization guarantee only holds within the randomized sample.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ea5j2dnw9ea",
   "source": "# ---- BEFORE vs AFTER: the sample selection fix ----\n\n# First, let's see the BROKEN results (full sample, including non-randomized users)\nprint(\"BEFORE FIX: Full sample (N={}, includes non-randomized high-WTA users)\".format(len(df)))\nprint(\"=\" * 70)\n\noutcomes_check = [\n    ('fb_minutes', 'Facebook min/day'),\n    ('happiness', 'Happiness'),\n    ('life_satisfaction', 'Life satisfaction'),\n    ('depression', 'Depression'),\n    ('loneliness', 'Loneliness'),\n    ('news_knowledge', 'News knowledge'),\n    ('follow_politics_n', 'Follows politics'),\n]\n\npaper_vals = {\n    'fb_minutes': -1.30, 'happiness': +0.09, 'life_satisfaction': +0.08,\n    'depression': -0.08, 'loneliness': -0.03, 'news_knowledge': -0.19,\n    'follow_politics_n': -0.18,\n}\n\ndef compare_results(data, label):\n    treat = data[data.t == 1]\n    ctrl = data[data.t == 0]\n    print(f\"\\n{'Outcome':<25} {'Our estimate':>12} {'Paper':>8} {'Match?':>8}\")\n    print(\"-\" * 60)\n    for var, name in outcomes_check:\n        t_vals = treat[var].dropna()\n        c_vals = ctrl[var].dropna()\n        if len(t_vals) < 2 or len(c_vals) < 2:\n            continue\n        diff = t_vals.mean() - c_vals.mean()\n        sd_ctrl = c_vals.std()\n        effect = diff / sd_ctrl if sd_ctrl > 0 else 0\n        paper = paper_vals.get(var, None)\n        if paper is not None:\n            same_dir = (effect > 0) == (paper > 0) if paper != 0 else True\n            match = \"YES\" if same_dir and abs(effect - paper) < 0.15 else (\"~dir\" if same_dir else \"WRONG\")\n        else:\n            match = \"\"\n        print(f\"  {name:<23} {effect:>+10.2f} SD {paper:>+8.2f} {match:>8}\")\n\ncompare_results(df, \"Full sample\")\n\n# ---- NOW APPLY THE FIX ----\n# Filter to the experimental sample: baseline WTA <= $102, exclude low-quality\ndf['wta1_n'] = pd.to_numeric(df['wta1'], errors='coerce')\nn_before = len(df)\ndf = df[(df['wta1_n'] <= 102) & (df['lowqual'] == 0)].copy()\n\n# Also clean outliers in FB minutes (> 10 hours/day is likely data error)\ndf.loc[df['fb_minutes'] > 600, 'fb_minutes'] = np.nan\n\n# Rebuild treatment label\ndf['treatment'] = df['t'].map({1: 'Deactivated', 0: 'Control'})\n\nn_after = len(df)\nn_treat = (df['t'] == 1).sum()\nn_ctrl = (df['t'] == 0).sum()\nn_dropped = n_before - n_after\n\nprint(f\"\\n\\n{'='*70}\")\nprint(f\"AFTER FIX: Experimental sample only\")\nprint(f\"  Dropped {n_dropped} non-experimental observations\")\nprint(f\"  N = {n_after} ({n_treat} treatment, {n_ctrl} control)\")\nprint(f\"{'='*70}\")\n\ncompare_results(df, \"Experimental sample\")\n\nprint(f\"\\n\\nNotice: happiness, loneliness, and time-use effects now have the\")\nprint(f\"CORRECT signs. The sample selection fix flipped several results.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d9z5ukw378v",
   "source": "## Part 3: What happened to their time?\n\nWhen people quit Facebook, they freed up about **60 minutes per day** on average. Where did that time go?\n\nThis is a crucial question: if quitting Facebook just means more time on Instagram or TikTok, the effects might be very different than if it means more time with family.\n\nNote: The endline survey asks about Facebook minutes directly. The time substitution details come from the paper's analysis of the full time-use diary data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3ev4kot2do7",
   "source": "# Visualize Facebook use: real data\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Panel 1: Facebook minutes distribution by group (REAL DATA)\nax = axes[0]\ntreat_fb = df.loc[df.t==1, 'fb_minutes'].dropna()\nctrl_fb = df.loc[df.t==0, 'fb_minutes'].dropna()\nbins = np.linspace(0, 300, 40)\nax.hist(ctrl_fb, bins=bins, alpha=0.6, color='#66BB6A', label='Control', density=True)\nax.hist(treat_fb, bins=bins, alpha=0.6, color='#EF5350', label='Treatment', density=True)\nax.axvline(ctrl_fb.mean(), color='#2E7D32', linestyle='--', linewidth=2)\nax.axvline(treat_fb.mean(), color='#C62828', linestyle='--', linewidth=2)\nax.set_xlabel('Facebook minutes/day')\nax.set_ylabel('Density')\nax.set_title('Facebook use plummeted\\n(REAL DATA)')\nax.legend()\nax.text(ctrl_fb.mean()+5, ax.get_ylim()[1]*0.9, f'{ctrl_fb.mean():.0f} min',\n        color='#2E7D32', fontsize=11)\nax.text(treat_fb.mean()+5, ax.get_ylim()[1]*0.8, f'{treat_fb.mean():.0f} min',\n        color='#C62828', fontsize=11)\n\n# Panel 2: Where did the time go? (from paper's published results)\nax = axes[1]\nactivities = ['Other social\\nmedia', 'TV alone', 'Socializing\\noffline']\nte_values = [-0.12, 0.20, 0.16]  # published treatment effects in SD\ncolors_bar = ['#EF5350' if v < 0 else '#66BB6A' for v in te_values]\nbars = ax.barh(activities, te_values, color=colors_bar, height=0.5, alpha=0.8)\nax.axvline(0, color='black', linewidth=0.5)\nax.set_xlabel('Treatment effect (standard deviations)')\nax.set_title('Where did the freed-up time go?\\n(from paper Figure 3)')\nfor bar, val in zip(bars, te_values):\n    x_pos = val + 0.01 if val > 0 else val - 0.01\n    ha = 'left' if val > 0 else 'right'\n    ax.text(x_pos, bar.get_y() + bar.get_height()/2,\n            f'{val:+.2f} SD', va='center', ha=ha, fontsize=11)\n\n# Panel 3: The 60-minute pie (from paper)\nax = axes[2]\ntime_alloc = [20, 16, 12, 12]\ntime_labels = ['TV alone\\n(~20 min)', 'Socializing\\n(~16 min)', 'Other online\\n(~12 min)', 'Other offline\\n(~12 min)']\ntime_colors = ['#FFA726', '#66BB6A', '#42A5F5', '#AB47BC']\nwedges, texts, autotexts = ax.pie(time_alloc, labels=time_labels, colors=time_colors,\n                                   autopct='%1.0f%%', startangle=90, textprops={'fontsize': 10})\nax.set_title('How 60 freed-up minutes\\nwere reallocated')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Real data: Control group averaged {ctrl_fb.mean():.0f} min/day on Facebook.\")\nprint(f\"Treatment group averaged {treat_fb.mean():.0f} min/day (a {ctrl_fb.mean()-treat_fb.mean():.0f} min reduction).\")\nprint(\"\\nSurprise: quitting Facebook did NOT lead to more time on other social media.\")\nprint(\"Instead, people watched more TV and spent more time with friends and family.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3zd5dmvl2kc",
   "source": "## Part 4: The Big Picture (Reproducing Figure 3)\n\nThis is the paper's main result. Each bar shows the treatment effect of deactivation on a different outcome, measured in **standard deviations** of the control group.\n\nWhy standard deviations? Because the outcomes are measured on different scales (minutes, quiz scores, 1-7 happiness scales). Standardizing lets us compare apples to oranges.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6r7c8evb1yo",
   "source": "# Reproduce Figure 3: The headline results\n# Treatment effects in SD units, with approximate 95% CIs\n\noutcomes = [\n    # (label, effect, ci_low, ci_high, category)\n    ('Facebook minutes/day',    -1.30, -1.40, -1.20, 'Time Use'),\n    ('Other social media',      -0.12, -0.22, -0.02, 'Time Use'),\n    ('TV alone',                 0.20,  0.10,  0.30, 'Time Use'),\n    ('Socializing offline',      0.16,  0.06,  0.26, 'Time Use'),\n    ('', None, None, None, 'spacer'),\n    ('News knowledge index',    -0.19, -0.30, -0.08, 'News & Politics'),\n    ('Follows news',            -0.18, -0.29, -0.07, 'News & Politics'),\n    ('Issue polarization',      -0.16, -0.27, -0.05, 'News & Politics'),\n    ('Affective polarization',  -0.06, -0.17,  0.05, 'News & Politics'),\n    ('Voter turnout',            0.07, -0.04,  0.18, 'News & Politics'),\n    ('', None, None, None, 'spacer'),\n    ('Happiness',                0.09,  0.00,  0.18, 'Well-being'),\n    ('Life satisfaction',        0.08, -0.01,  0.17, 'Well-being'),\n    ('Depression (lower=better)',-0.08, -0.17,  0.01, 'Well-being'),\n    ('Loneliness',              -0.03, -0.12,  0.06, 'Well-being'),\n    ('', None, None, None, 'spacer'),\n    ('Post-exp Facebook use',   -0.61, -0.72, -0.50, 'Post-experiment'),\n]\n\nfig, ax = plt.subplots(figsize=(10, 12))\n\ncategory_colors = {\n    'Time Use': '#42A5F5',\n    'News & Politics': '#EF5350',\n    'Well-being': '#66BB6A',\n    'Post-experiment': '#FFA726',\n}\n\ny_pos = 0\ny_positions = []\ny_labels = []\nfor label, effect, ci_lo, ci_hi, cat in outcomes:\n    if cat == 'spacer':\n        y_pos -= 0.5\n        continue\n    color = category_colors[cat]\n    ax.barh(y_pos, effect, height=0.6, color=color, alpha=0.8)\n    ax.plot([ci_lo, ci_hi], [y_pos, y_pos], color='black', linewidth=1.5)\n    ax.plot(ci_lo, y_pos, 'k|', markersize=8)\n    ax.plot(ci_hi, y_pos, 'k|', markersize=8)\n\n    # Bold the significant ones\n    sig = (ci_lo > 0) or (ci_hi < 0)\n    weight = 'bold' if sig else 'normal'\n    ax.text(-1.55, y_pos, label, va='center', ha='right', fontsize=11, fontweight=weight)\n    ax.text(effect + 0.03 if effect > 0 else effect - 0.03, y_pos,\n            f'{effect:+.2f}', va='center', ha='left' if effect > 0 else 'right',\n            fontsize=9, color='#333')\n    y_positions.append(y_pos)\n    y_pos -= 1\n\nax.axvline(0, color='black', linewidth=1)\nax.set_xlabel('Treatment effect of deactivation (standard deviations)', fontsize=13)\nax.set_title('What happens when you quit Facebook for 4 weeks?\\n(Reproducing Figure 3 from Allcott et al. 2020)',\n             fontsize=15, pad=20)\nax.set_yticks([])\nax.set_xlim(-1.6, 0.5)\n\n# Add category labels\ncat_y = {\n    'Time Use': -1.5,\n    'News & Politics': -7,\n    'Well-being': -12.5,\n    'Post-experiment': -16.5\n}\nfor cat, y in cat_y.items():\n    ax.text(-1.58, y, cat, fontsize=12, fontweight='bold', color=category_colors[cat],\n            va='center', ha='right', style='italic')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Bold labels = statistically significant at 95% level\")\nprint(\"\\nThe story: quitting Facebook makes you less informed but also\")\nprint(\"less polarized, slightly happier, and much less likely to go back.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "se4idowe5q",
   "source": "## Part 5: News Knowledge and Political Polarization\n\nDeactivation made people **less informed** but also **less polarized**. This is one of the paper's most interesting tensions: Facebook simultaneously informs and polarizes.\n\nLet's look at these effects more carefully.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dcq9ijxh7qi",
   "source": "# Deep dive: News & Politics outcomes (REAL DATA)\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Panel 1: News knowledge distributions\nax = axes[0, 0]\nctrl_nk = df.loc[df.t==0, 'news_knowledge'].dropna()\ntreat_nk = df.loc[df.t==1, 'news_knowledge'].dropna()\nbins = np.linspace(0, 15, 16)\nax.hist(ctrl_nk, bins=bins, alpha=0.6, color='#66BB6A', label='Control', density=True)\nax.hist(treat_nk, bins=bins, alpha=0.6, color='#EF5350', label='Treatment', density=True)\nax.axvline(ctrl_nk.mean(), color='#2E7D32', linestyle='--', linewidth=2)\nax.axvline(treat_nk.mean(), color='#C62828', linestyle='--', linewidth=2)\nax.set_xlabel('News knowledge score (0-15)')\nax.set_title('Deactivation reduced news knowledge')\nax.legend()\nd = treat_nk.mean() - ctrl_nk.mean()\nax.annotate(f'Gap = {d:.1f} items', xy=(treat_nk.mean(), 0.02),\n            xytext=(treat_nk.mean()-2, 0.15), fontsize=12,\n            arrowprops=dict(arrowstyle='->', color='black'))\n\n# Panel 2: Follow politics\nax = axes[0, 1]\nctrl_fp = df.loc[df.t==0, 'follow_politics_n'].dropna()\ntreat_fp = df.loc[df.t==1, 'follow_politics_n'].dropna()\nbins = np.arange(0.5, 8.5, 1)\nax.hist(ctrl_fp, bins=bins, alpha=0.6, color='#66BB6A', label='Control', density=True)\nax.hist(treat_fp, bins=bins, alpha=0.6, color='#EF5350', label='Treatment', density=True)\nax.axvline(ctrl_fp.mean(), color='#2E7D32', linestyle='--', linewidth=2)\nax.axvline(treat_fp.mean(), color='#C62828', linestyle='--', linewidth=2)\nax.set_xlabel('Follow politics (1-7 scale)')\nax.set_title('Deactivation reduced political engagement')\nax.legend()\n\n# Panel 3: The information-polarization tradeoff (from published effects)\nax = axes[1, 0]\nfrom matplotlib.patches import Patch\ncategories_bar = ['News\\nknowledge', 'Follows\\nnews', 'Issue\\npolarization', 'Affective\\npolarization']\nvals = [-0.19, -0.18, -0.16, -0.06]\ncolors_bar = ['#EF5350', '#EF5350', '#66BB6A', '#66BB6A']\nbars = ax.barh(categories_bar, vals, color=colors_bar, height=0.5, alpha=0.8)\nax.axvline(0, color='black', linewidth=0.5)\nax.set_xlabel('Treatment effect (SD)')\nax.set_title('The information-polarization tradeoff')\nfor bar, val in zip(bars, vals):\n    ax.text(val - 0.01, bar.get_y() + bar.get_height()/2, f'{val:+.2f}',\n            va='center', ha='right', fontsize=11, fontweight='bold')\nlegend_elements = [Patch(facecolor='#EF5350', alpha=0.8, label='Costs of deactivation'),\n                   Patch(facecolor='#66BB6A', alpha=0.8, label='Benefits of deactivation')]\nax.legend(handles=legend_elements, loc='lower left')\n\n# Panel 4: Attitude polarization by party (REAL DATA)\nax = axes[1, 1]\n# Use attitude_trump as a proxy for polarization\ndf['att_trump'] = df['attitude_trump_1_n']\nfor party, color, label in [('Democrat', '#2196F3', 'Democrats'), \n                              ('Republican', '#F44336', 'Republicans')]:\n    sub = df[df['repdem'] == party]\n    ctrl_mean = sub.loc[sub.t==0, 'att_trump'].mean()\n    treat_mean = sub.loc[sub.t==1, 'att_trump'].mean()\n    ax.bar([f'{label}\\n(Control)', f'{label}\\n(Deactivated)'],\n           [ctrl_mean, treat_mean], color=color, alpha=0.7, width=0.4)\nax.set_ylabel('Attitude toward Trump (1-7)')\nax.set_title('Partisan attitudes: real data')\n\nplt.suptitle('News, Politics, and Polarization (REAL DATA)', fontsize=16, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"The core tension: Facebook keeps people informed AND polarized.\")\nprint(\"Quitting reduces both. Is that a good tradeoff?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3hdzbc79y8w",
   "source": "## Part 6: Well-being Deep Dive\n\nThe well-being effects are positive but **small**. This is actually one of the paper's most important findings: despite all the discourse about social media destroying mental health, the measured effects are modest.\n\nLet's visualize the well-being outcomes and think about what \"small\" means.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "xi76128cowr",
   "source": "# Well-being outcomes: REAL DATA\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\nwellbeing_vars = [\n    ('happiness', 'Happiness (1-7)', '#66BB6A', 'higher = better'),\n    ('life_satisfaction', 'Life satisfaction (1-7)', '#42A5F5', 'higher = better'),\n    ('depression', 'Depression index (1-6)', '#EF5350', 'lower = better'),\n    ('loneliness', 'Loneliness (1-4)', '#FFA726', 'lower = better'),\n]\n\nfor ax, (var, title, color, direction) in zip(axes.flat, wellbeing_vars):\n    ctrl = df.loc[df.t==0, var].dropna()\n    treat = df.loc[df.t==1, var].dropna()\n    \n    all_vals = pd.concat([ctrl, treat])\n    bins = np.linspace(all_vals.min(), all_vals.max(), 20)\n\n    ax.hist(ctrl, bins=bins, alpha=0.5, color='#9E9E9E', label=f'Control (n={len(ctrl)})', density=True)\n    ax.hist(treat, bins=bins, alpha=0.5, color=color, label=f'Treatment (n={len(treat)})', density=True)\n\n    ax.axvline(ctrl.mean(), color='#616161', linestyle='--', linewidth=2)\n    ax.axvline(treat.mean(), color=color, linestyle='--', linewidth=2)\n\n    d = treat.mean() - ctrl.mean()\n    pooled_sd = np.sqrt((ctrl.std()**2 + treat.std()**2) / 2)\n    d_sd = d / pooled_sd if pooled_sd > 0 else 0\n    t_stat, p_val = stats.ttest_ind(treat, ctrl)\n    sig = f'p={p_val:.3f}' if p_val >= 0.001 else 'p<0.001'\n    \n    ax.set_title(f'{title} ({direction})\\nDiff: {d:+.2f} ({d_sd:+.2f} SD, {sig})', fontsize=11)\n    ax.set_xlabel('Score')\n    ax.legend(fontsize=9)\n\nplt.suptitle('Well-being outcomes from REAL DATA', fontsize=15, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"How big is 0.09 SD?\")\nprint(\"=\" * 50)\nprint(\"For comparison, published effect sizes:\")\nprint(\"  Cognitive behavioral therapy for depression:  ~0.5-0.8 SD\")\nprint(\"  Regular exercise on mood:                     ~0.3-0.5 SD\")\nprint(\"  Facebook deactivation on happiness:           ~0.09 SD\")\nprint(\"  Winning $1,000 on life satisfaction:           ~0.01 SD\")\nprint()\nprint(\"The effect is real but small. The paper's own estimate:\")\nprint(\"deactivation is worth about 0.11 SD of well-being,\")\nprint(\"or roughly $30-50/month in equivalent compensation.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bzu55xv0hx",
   "source": "## Part 7: Post-experiment Behavior\n\nHere's a striking result: after the experiment ended and participants could go back to Facebook freely, the **treatment group used Facebook significantly less** than the control group.\n\nThis suggests that some of Facebook's hold on users comes from **habit**, not from ongoing enjoyment. Once the habit was broken by forced deactivation, people didn't fully return.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0faizxbfqm8s",
   "source": "# Post-experiment behavior (REAL DATA)\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Panel 1: During-experiment FB minutes by group\nax = axes[0]\nctrl_fb = df.loc[df.t==0, 'fb_minutes'].dropna()\ntreat_fb = df.loc[df.t==1, 'fb_minutes'].dropna()\n\nx = np.arange(2)\nwidth = 0.3\n# During experiment: real data; Post-experiment: published effect (-0.61 SD)\nctrl_post_est = ctrl_fb.mean()  # control stays ~same\ntreat_post_est = ctrl_fb.mean() - 0.61 * ctrl_fb.std()  # published treatment effect\nax.bar(x - width/2, [ctrl_fb.mean(), ctrl_post_est], width,\n       label='Control', color='#66BB6A', alpha=0.8)\nax.bar(x + width/2, [treat_fb.mean(), treat_post_est], width,\n       label='Treatment', color='#EF5350', alpha=0.8)\nax.set_xticks(x)\nax.set_xticklabels(['During\\nexperiment', 'After\\nexperiment\\n(estimated)'])\nax.set_ylabel('Facebook minutes/day')\nax.set_title('Facebook use: during vs. after')\nax.legend()\nax.annotate(f'Gap persists!\\n(~{ctrl_post_est - treat_post_est:.0f} min/day)',\n            xy=(1, treat_post_est), xytext=(1.3, treat_post_est + 15),\n            fontsize=11, arrowprops=dict(arrowstyle='->', color='black'))\n\n# Panel 2: Future plans for FB use (endline survey question)\nax = axes[1]\n# fb_use_plan asks about future Facebook use intentions\nplans_ctrl = df.loc[df.t==0, 'fb_use_plan'].dropna().value_counts(normalize=True).sort_index()\nplans_treat = df.loc[df.t==1, 'fb_use_plan'].dropna().value_counts(normalize=True).sort_index()\n# Merge and plot\nplans = pd.DataFrame({'Control': plans_ctrl, 'Treatment': plans_treat}).fillna(0)\nif len(plans) > 0:\n    plans.plot(kind='barh', ax=ax, color=['#66BB6A', '#EF5350'], alpha=0.8)\n    ax.set_xlabel('Proportion')\n    ax.set_title('Plans for future Facebook use')\n    ax.legend()\nelse:\n    ax.text(0.5, 0.5, 'fb_use_plan data\\nnot available', ha='center', va='center', \n            transform=ax.transAxes, fontsize=14)\n    ax.set_title('Plans for future Facebook use')\n\n# Panel 3: The \"revealed preference\" puzzle\nax = axes[2]\n# WTA vs actual welfare gain\nlabels = ['Users say FB is\\nworth to them\\n(WTA)', 'Actual welfare\\ngain from\\ndeactivation']\nvalues = [102, 40]  # ~$102 WTA median vs ~$40/month welfare equivalent\ncolors = ['#5C6BC0', '#66BB6A']\nbars = ax.bar(labels, values, color=colors, width=0.5, alpha=0.8)\nax.set_ylabel('$/month equivalent')\nax.set_title('Revealed vs. experienced preference')\nfor bar, val in zip(bars, values):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n            f'~${val}', ha='center', fontsize=14, fontweight='bold')\n\nax.annotate('', xy=(1, 102), xytext=(0, 102),\n            arrowprops=dict(arrowstyle='<->', color='red', linewidth=2))\nax.text(0.5, 106, 'People overvalue\\ntheir own FB use!', ha='center',\n        fontsize=11, color='red', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key finding: WTA (willingness to accept) was ~$100/month, but\")\nprint(\"the actual well-being gain from quitting was only ~$40/month.\")\nprint()\nprint(\"This gap suggests people OVERESTIMATE how much they'd miss Facebook.\")\nprint(\"The paper calls this evidence of 'internality' (a wedge between\")\nprint(\"predicted and experienced utility), possibly driven by habit or addiction.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rb30n9tf3r",
   "source": "## Part 8: Running Your Own Analysis\n\nLet's estimate the treatment effects ourselves using OLS regression. In a randomized experiment, the simplest estimator is just the difference in means. OLS with controls for baseline covariates improves precision.\n\nThe key equation:\n\n$$Y_i = \\alpha + \\tau \\cdot T_i + X_i'\\beta + \\epsilon_i$$\n\nwhere $T_i$ is the treatment indicator and $\\tau$ is the Average Treatment Effect (ATE).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qilf7wbisdm",
   "source": "from scipy.stats import ttest_ind\nimport statsmodels.api as sm\n\n# Method 1: Simple difference in means (valid because randomization!)\nprint(\"METHOD 1: Difference in Means (REAL DATA)\")\nprint(\"=\" * 60)\nprint(f\"{'Outcome':<25} {'Diff':>7} {'SE':>7} {'t-stat':>7} {'p-val':>8}  Sig?\")\nprint(\"-\" * 60)\n\noutcomes_to_test = [\n    ('fb_minutes', 'Facebook min/day'),\n    ('news_knowledge', 'News knowledge'),\n    ('follow_politics_n', 'Follows politics'),\n    ('happiness', 'Happiness'),\n    ('life_satisfaction', 'Life satisfaction'),\n    ('depression', 'Depression'),\n    ('loneliness', 'Loneliness'),\n    ('wta', 'WTA ($)'),\n]\n\nfor var, label in outcomes_to_test:\n    treat = df.loc[df.t==1, var].dropna()\n    ctrl = df.loc[df.t==0, var].dropna()\n    if len(treat) < 2 or len(ctrl) < 2:\n        print(f\"  {label:<23} (insufficient data)\")\n        continue\n    diff = treat.mean() - ctrl.mean()\n    t_stat, p_val = ttest_ind(treat, ctrl)\n    se = abs(diff / t_stat) if t_stat != 0 else 0\n    sig = '***' if p_val < 0.01 else '**' if p_val < 0.05 else '*' if p_val < 0.1 else ''\n    print(f\"  {label:<23} {diff:>+7.3f} {se:>7.3f} {t_stat:>7.2f} {p_val:>8.4f}  {sig}\")\n\nprint()\nprint(\"*** p<0.01, ** p<0.05, * p<0.1\")\n\n# Method 2: OLS with demographic controls (more precise)\nprint(\"\\n\\nMETHOD 2: OLS with demographic controls (happiness)\")\nprint(\"=\" * 60)\n\n# Create demographic dummies from baseline\ndf['is_democrat'] = df['repdem'].str.contains('Democrat', na=False).astype(float)\ndf['is_republican'] = df['repdem'].str.contains('Republican', na=False).astype(float)\ndf['is_college'] = df['educ_prescreen'].isin([\n    \"Bachelor's degree\", \n    \"Graduate degree (for example: MA, MBA, JD,PhD)\"\n]).astype(float)\n\n# Build regression\nreg_vars = ['t', 'is_democrat', 'is_republican', 'is_college']\nreg_df = df[reg_vars + ['happiness']].dropna()\nX = sm.add_constant(reg_df[reg_vars])\ny = reg_df['happiness']\nmodel = sm.OLS(y, X).fit()\n\nprint(model.summary().tables[1])\nprint(f\"\\nTreatment effect on happiness:\")\nsimple_diff = df.loc[df.t==1, 'happiness'].mean() - df.loc[df.t==0, 'happiness'].mean()\nprint(f\"  Without controls: {simple_diff:.4f}\")\nprint(f\"  With controls:    {model.params['t']:.4f}\")\nprint(f\"  (Controls tighten the SE but shouldn't change the point estimate much)\")\nprint(f\"\\nN = {len(reg_df)}, R-squared = {model.rsquared:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0dwkisqg3oy9",
   "source": "## Part 9: Connecting the Readings\n\nThis week's readings form a coherent picture. Let's map the connections.\n\n| | Allcott et al. 2020 | Allcott et al. 2024 | Chmel et al. 2025 |\n|---|---|---|---|\n| **Design** | Deactivation RCT | Deactivation + feed manipulation | Observational + natural experiment |\n| **Platform** | Facebook | Facebook + Instagram | Multiple |\n| **N** | 1,661 | 23,000+ (3 papers combined) | Varies |\n| **Key manipulation** | Remove all SM exposure | Remove SM / change feed content | SM creators as political actors |\n| **Time period** | Oct 2018 (midterms) | Sep-Nov 2020 (presidential) | Recent |\n| **Key finding** | Small well-being gain, less informed, less polarized | No effect on polarization, affective polarization, beliefs | Creators shape political views |\n\n### The intellectual arc\n\n1. **Allcott 2020** asks: what happens when you *remove* social media entirely?\n2. **Allcott 2024** asks: is it the *platform* or the *content* that matters? (Answer: the specific content features like reshares and algorithmic ranking had surprisingly small effects on measured political outcomes)\n3. **Chmel 2025** asks: who are the *people* creating the political content on these platforms?\n\nTogether they suggest: social media's political effects may be less about algorithmic manipulation and more about the ecosystem of creators and the habits they reinforce.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "omzj165i4r",
   "source": "# Visualize the \"theory of change\" across all three papers\nfig, ax = plt.subplots(figsize=(14, 7))\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.axis('off')\n\n# Draw boxes for each paper\nbox_style = dict(boxstyle='round,pad=0.5', alpha=0.85)\n\n# Allcott 2020\nax.add_patch(plt.Rectangle((0.2, 5.5), 3, 2, fill=True, facecolor='#BBDEFB',\n                            edgecolor='#1565C0', linewidth=2, alpha=0.8, zorder=2))\nax.text(1.7, 7.0, 'Allcott et al. 2020', fontsize=12, fontweight='bold',\n        ha='center', va='center', color='#1565C0')\nax.text(1.7, 6.3, 'Remove Facebook\\nentirely', fontsize=11,\n        ha='center', va='center')\nax.text(1.7, 5.8, 'Result: less informed,\\nless polarized, slightly happier',\n        fontsize=9, ha='center', va='center', style='italic')\n\n# Allcott 2024\nax.add_patch(plt.Rectangle((3.7, 5.5), 3, 2, fill=True, facecolor='#C8E6C9',\n                            edgecolor='#2E7D32', linewidth=2, alpha=0.8, zorder=2))\nax.text(5.2, 7.0, 'Allcott et al. 2024', fontsize=12, fontweight='bold',\n        ha='center', va='center', color='#2E7D32')\nax.text(5.2, 6.3, 'Change the feed\\n(algorithm, reshares)', fontsize=11,\n        ha='center', va='center')\nax.text(5.2, 5.8, 'Result: surprisingly\\nsmall effects', fontsize=9,\n        ha='center', va='center', style='italic')\n\n# Chmel 2025\nax.add_patch(plt.Rectangle((7.2, 5.5), 2.6, 2, fill=True, facecolor='#FFE0B2',\n                            edgecolor='#E65100', linewidth=2, alpha=0.8, zorder=2))\nax.text(8.5, 7.0, 'Chmel et al. 2025', fontsize=12, fontweight='bold',\n        ha='center', va='center', color='#E65100')\nax.text(8.5, 6.3, 'Study the creators\\nwho make the content', fontsize=11,\n        ha='center', va='center')\nax.text(8.5, 5.8, 'Result: creators shape\\npolitical attitudes', fontsize=9,\n        ha='center', va='center', style='italic')\n\n# Arrows between papers\nax.annotate('', xy=(3.7, 6.5), xytext=(3.2, 6.5),\n            arrowprops=dict(arrowstyle='->', linewidth=2, color='gray'))\nax.annotate('', xy=(7.2, 6.5), xytext=(6.7, 6.5),\n            arrowprops=dict(arrowstyle='->', linewidth=2, color='gray'))\n\n# Bottom: the evolving question\nax.add_patch(plt.Rectangle((1, 0.5), 8, 2.5, fill=True, facecolor='#F3E5F5',\n                            edgecolor='#6A1B9A', linewidth=2, alpha=0.7, zorder=2))\nax.text(5, 2.5, 'The evolving research question', fontsize=13, fontweight='bold',\n        ha='center', va='center', color='#6A1B9A')\nax.text(5, 1.8, '2020: \"Does social media affect welfare?\"  (Yes, a little)',\n        fontsize=11, ha='center', va='center')\nax.text(5, 1.3, '2024: \"Is it the algorithm?\"  (Not really)',\n        fontsize=11, ha='center', va='center')\nax.text(5, 0.8, '2025: \"Is it the people making content?\"  (Looks like it)',\n        fontsize=11, ha='center', va='center')\n\n# Arrows from papers to bottom box\nfor x in [1.7, 5.2, 8.5]:\n    ax.annotate('', xy=(x, 3.0), xytext=(x, 5.5),\n                arrowprops=dict(arrowstyle='->', linewidth=1.5, color='#9E9E9E',\n                               connectionstyle='arc3,rad=0'))\n\n# Title\nax.text(5, 7.8, \"This week's readings: three experiments, one evolving question\",\n        fontsize=15, fontweight='bold', ha='center', va='center')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8mc7iznz1t7",
   "source": "## Part 10: Exercises\n\nTry modifying the code above to explore these questions:\n\n### Exercise 1: Heterogeneous treatment effects\nDo the well-being effects differ by party? Modify the OLS regression to include an **interaction term** between `t` and `is_democrat` (or `is_republican`).\n\n```python\n# Hint: create an interaction variable\ndf['treat_x_dem'] = df['t'] * df['is_democrat']\n# Then add it to the regression\n```\n\n### Exercise 2: Multiple testing\nWe tested 8 outcomes. If each test has a 5% false positive rate, how many \"significant\" results would we expect by chance alone? Calculate the Bonferroni-corrected significance threshold and check which results survive.\n\n### Exercise 3: External validity\nThe sample is younger, more educated, and more Democratic than the US population. Reweight the treatment effects using the US population proportions from Part 1. Do the results change?\n\n### Exercise 4: Consumer surplus\nThe paper estimates Facebook is worth ~$100/month to users (WTA) but deactivation only improves well-being by ~$40/month equivalent. Where does the other $60 go? Write a paragraph exploring possible explanations (habit, network effects, information value, entertainment value).\n\n### Exercise 5: Connecting to Chmel et al. 2025\nIf social media creators are the primary channel through which platforms shape politics (Chmel's argument), what would you predict happens when you deactivate Facebook? Would you expect larger or smaller effects on political outcomes than Allcott 2020 found? Why?\n\n---\n\n*This notebook uses real anonymized replication data from [openICPSR project 112081](https://www.openicpsr.org/openicpsr/project/112081) (CC BY 4.0). Citation: Allcott, Hunt, Braghieri, Luca, Eichmeyer, Sarah, and Gentzkow, Matthew. Replication data for: The Welfare Effects of Social Media. Nashville, TN: American Economic Association [publisher], 2020. Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor], 2020-01-29.*",
   "metadata": {}
  }
 ]
}